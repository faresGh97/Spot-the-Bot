{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c44edbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tqdm\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb29c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_ws_corpus(input_path,output_path):\n",
    "#     fin = open(input_path, \"rt\",encoding = 'utf-8')\n",
    "#     fout = open(output_path, \"wt\",encoding = 'utf-8')\n",
    "#     for line in fin:\n",
    "#         fout.write(re.sub('\\s+',' ',line))\n",
    "#     fin.close()\n",
    "#     fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e2fc698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_ws_corpus(\"english_newlit_corpus.txt\",\"English_corpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "408a9036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse.linalg import svds\n",
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd \n",
    "from gensim.models import Word2Vec\n",
    "from graph_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68a207a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer():\n",
    "    def __init__(self, corp_path):\n",
    "        self.corp_path = corp_path\n",
    "    \n",
    "    def upload_corp(self):\n",
    "        with open(self.corp_path, 'r',encoding = 'utf-8') as f:\n",
    "            self.corp = list(set(f.read().split('\\n')[:-1]))\n",
    "        #self.corp = [text.replace('$$$', ' ') for text in self.corp]\n",
    "    \n",
    "    \n",
    "    def log(self, part):\n",
    "        clear_output(wait=True)\n",
    "        print(f'{part} is processing')\n",
    "        \n",
    "    def make_tf_idf_matrix(self, token_pattern=None, min_df = 1, max_df=None, use_idf = True):\n",
    "        if token_pattern:\n",
    "            self.tfidf = TfidfVectorizer(analyzer='word', min_df=min_df, token_pattern=token_pattern, use_idf=use_idf)\n",
    "        else:\n",
    "            self.tfidf = TfidfVectorizer(analyzer='word', min_df=min_df, use_idf=use_idf)\n",
    "            \n",
    "        self.W = self.tfidf.fit_transform(self.corp)\n",
    "        self.words_list = self.tfidf.get_feature_names_out()\n",
    "        \n",
    "    def make_svd(self, output_folder, k=6 ):\n",
    "#         self.u, self.sigma, self.vt = svds(self.W, k)\n",
    "        \n",
    "#         self.descending_order_of_inds = np.flip(np.argsort(self.sigma))\n",
    "#         self.u = self.u[:,self.descending_order_of_inds]\n",
    "#         self.vt = self.vt[self.descending_order_of_inds,:]\n",
    "#         self.sigma = np.diag(self.sigma[self.descending_order_of_inds])\n",
    "        self.u, self.sigma, self.vT = svds(self.W, k)\n",
    "        self.descending_order_of_inds = np.argsort(-self.sigma)\n",
    "        \n",
    "        self.u = self.u[:, self.descending_order_of_inds]\n",
    "        self.sigma = np.diag(self.sigma[self.descending_order_of_inds])\n",
    "        self.vT = self.vT[self.descending_order_of_inds, :]\n",
    "\n",
    "        #Checking that sizes are ok\n",
    "        #assert self.sigma.shape == (k,)\n",
    "        #assert swlf.vt.shape == (k, self.W.shape[1])\n",
    "        #assert swlf.u.shape == (self.W.shape[0], k)\n",
    "        print (self.W.shape)\n",
    "        print (self.sigma.shape)\n",
    "        print (self.vT.shape)\n",
    "        print (self.u.shape)\n",
    "        \n",
    "        self.embedded_matrix = self.sigma@self.vT\n",
    "        #self.embedded_matrix = np.dot(np.diag(self.sigma), self.vt).T\n",
    "        self.words_embedding_dict = dict(zip(self.words_list, self.embedded_matrix.T))\n",
    "        #self.words_embedding_dict = dict(zip(self.words_list, self.embedded_matrix))\n",
    "        \n",
    "#         with open(output_folder+'/' + str(k) + '_sigma_vt.npy', 'wb') as f:\n",
    "#             #np.save(f, np.dot(np.diag(self.sigma), self.vt).T)\n",
    "#             np.save(f, self.embedded_matrix.T)\n",
    "#         with open(output_folder+'/' +  str(k) + '_sigma.npy', 'wb') as f:\n",
    "#             np.save(f, self.sigma)\n",
    "#         with open(output_folder+'/' +  str(k) + '_u.npy', 'wb') as f:\n",
    "#             np.save(f, self.u)\n",
    "#         with open(output_folder+'/' +  str(k) + '_vt.npy', 'wb') as f:\n",
    "#             np.save(f, self.vT)\n",
    "            \n",
    "        self.save_word_embedding(k)\n",
    "    \n",
    "    \n",
    "    def get_emb_dict(self):\n",
    "        \n",
    "        self.log('Upload')\n",
    "        self.upload_corp()\n",
    "        self.log('TfIdf')\n",
    "        self.make_tf_idf_matrix()\n",
    "        self.log('SVD')\n",
    "        self.make_svd('./Matrixes')\n",
    "        \n",
    "        return self.words_embedding_dict\n",
    "    \n",
    "    def save_word_embedding(self, shape):\n",
    "        \n",
    "        dict_ = {}\n",
    "        for key, value in self.words_embedding_dict.items():\n",
    "            dict_[key] = value.tolist()\n",
    "        save_name = \"English_WordVectorDict_SVD\" + str(shape) + \".json\"\n",
    "        with open(save_name, 'w') as f:\n",
    "            json.dump(dict_, f)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d32d3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD is processing\n",
      "(10886, 275618)\n",
      "(6, 6)\n",
      "(6, 275618)\n",
      "(10886, 6)\n"
     ]
    }
   ],
   "source": [
    "vect = Vectorizer('./english_newlit_corpus.txt')\n",
    "emb_dict = vect.get_emb_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67460d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(corp_path):\n",
    "    with open(corp_path, 'r',encoding = 'utf-8') as f:\n",
    "        corp = list(set(f.read().split('\\n')[:-1]))\n",
    "    #corp = [text.replace('$$$', ' ') for text in corp]\n",
    "    corp = [text.split() for text in corp]\n",
    "    return corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fac681bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_corpus('english_newlit_corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b2b05bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10886"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cd2b21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 6\n",
    "model = Word2Vec(sentences=documents, vector_size=dimension, min_count=1)\n",
    "model.save(\"word2vec_Englsih.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0c0d304",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {key : model.wv[key] for key in model.wv.key_to_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "878f91dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dictionary(dictionary,shape):\n",
    "    for key, value in dictionary.items():\n",
    "        dictionary[key] = value.tolist()\n",
    "    save_name = \"English_WordVectorDict_CBOW\" + str(shape) + \".json\"\n",
    "    with open(save_name, 'w') as f:\n",
    "        json.dump(dictionary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9eb860b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dictionary( dictionary, dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50a00982",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = './english_newlit_corpus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72d0ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_vector(text, dictionary, n, m):\n",
    "    text = text.strip().split()\n",
    "    text_vectors = []\n",
    "    text_words = []\n",
    "\n",
    "    for i in range(len(text) - n + 1):\n",
    "        gram_vec = []\n",
    "        words = []\n",
    "        for word in text[i:i+n]:\n",
    "            if word not in dictionary:\n",
    "                gram_vec = []\n",
    "                break\n",
    "            vec_ = dictionary[word][:m]\n",
    "            gram_vec.append(vec_)\n",
    "            words.append(word)\n",
    "        if len(gram_vec) != n or len(gram_vec[-1]) != m:\n",
    "            continue\n",
    "        text_vectors.append(np.array(gram_vec).flatten())\n",
    "        text_words.append(' '.join(words))\n",
    "    text_dict = dict(zip(text_words, text_vectors))\n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "393b1374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_corpus_with_ngrams(corpus_path: str,\n",
    "                                   separator: str,\n",
    "                                   word_vector_dict: dict, n : int , m : int  ) -> np.ndarray:\n",
    "    colnames=[ 'Text'] \n",
    "    df_corpus = pd.read_csv(corpus_path, names =colnames, header=None)\n",
    "    corpus = df_corpus['Text'].tolist()\n",
    "    vectorized_corpus = list()\n",
    "    for text_index, text in enumerate(tqdm(corpus)):\n",
    "        vectorized_text = dict()\n",
    "        if not text:\n",
    "            continue\n",
    "        vectorized_t = convert_text_to_vector(text, word_vector_dict,n,m)\n",
    "        vectorized_text['document_index'] = text_index\n",
    "        vectorized_text['vectorized_text'] = vectorized_t\n",
    "        vectorized_corpus.append(vectorized_text)\n",
    "    return np.array(vectorized_corpus, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44fa2024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How long is the n-gram\n",
    "n = 1\n",
    "#How long is a vector for every word\n",
    "m = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55f2b0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('English_WordVectorDict_CBOW6.json', encoding='utf-8') as f:\n",
    "    emb_dic = json.load(f)\n",
    "#print(CBOW_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe065df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11046/11046 [24:41<00:00,  7.46it/s] \n"
     ]
    }
   ],
   "source": [
    "vectorized_corpus = vectorize_corpus_with_ngrams(corpus_path,\n",
    "                                               ' ',\n",
    "                                               emb_dic,n,int(m/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c6301ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"English_vectorized_corpus_CBOW_1gram_6\", vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "909db906",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"English_WordVectorDict_SVD6.json\", encoding='utf-8') as f:\n",
    "    emb_dic = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e88ed259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11046/11046 [26:03<00:00,  7.07it/s] \n"
     ]
    }
   ],
   "source": [
    "vectorized_corpus = vectorize_corpus_with_ngrams(corpus_path,\n",
    "                                               ' ',\n",
    "                                               emb_dic,n,int(m/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed48b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"English_vectorized_corpus_SVD_1gram_6\", vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38237c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "At_VECTORIZED_CORPUS = \"./English_vectorized_corpus_SVD_1gram_6.npy\"\n",
    "at_corpus = np.load(At_VECTORIZED_CORPUS, allow_pickle=True)\n",
    "at_corpus = at_corpus.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e0068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(at_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308539ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = create_vertices(at_corpus, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a430676",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa67c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = GG(vertices)\n",
    "gg.create_gabriel_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77b1679c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnglish_Gabriel_Graph_SVD_1gram_6.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outp:\n\u001b[1;32m----> 2\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(\u001b[43mgg\u001b[49m, outp, pickle\u001b[38;5;241m.\u001b[39mHIGHEST_PROTOCOL)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gg' is not defined"
     ]
    }
   ],
   "source": [
    "with open('English_Gabriel_Graph_SVD_1gram_6.pkl', 'wb') as outp:\n",
    "    pickle.dump(gg, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d185103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "At_VECTORIZED_CORPUS = \"./English_vectorized_corpus_CBOW_1gram_6.npy\"\n",
    "at_corpus = np.load(At_VECTORIZED_CORPUS, allow_pickle=True)\n",
    "at_corpus = at_corpus.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26112a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = create_vertices(at_corpus, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa594311",
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = GG(vertices)\n",
    "gg.create_gabriel_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99e724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('English_Gabriel_Graph_CBOW_1gram_6.pkl', 'wb') as outp:\n",
    "    pickle.dump(gg, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8b42961",
   "metadata": {},
   "outputs": [],
   "source": [
    "At_VECTORIZED_CORPUS = \"./English_vectorized_corpus_CBOW_1gram_6.npy\"\n",
    "at_corpus = np.load(At_VECTORIZED_CORPUS, allow_pickle=True)\n",
    "at_corpus = at_corpus.tolist()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f63808e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(at_corpus)/100)\n",
    "# printing n elements from list\n",
    "test = random.choices(at_corpus, k=n)\n",
    "np.save(\"English_vectorized_corpus_sample10%_CBOW_1gram_6\", np.array(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1877c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
