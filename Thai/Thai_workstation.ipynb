{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9e876bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse.linalg import svds\n",
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import io \n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68e0b66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26cc0222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_vector(text, dictionary, n, m):\n",
    "    text = text.strip().split()\n",
    "    text_vectors = []\n",
    "    text_words = []\n",
    "\n",
    "    for i in range(len(text) - n + 1):\n",
    "        gram_vec = []\n",
    "        words = []\n",
    "        for word in text[i:i+n]:\n",
    "            if word not in dictionary:\n",
    "                gram_vec = []\n",
    "                break\n",
    "            vec_ = dictionary[word][:m]\n",
    "            gram_vec.append(vec_)\n",
    "            words.append(word)\n",
    "        if len(gram_vec) != n or len(gram_vec[-1]) != m:\n",
    "            continue\n",
    "        text_vectors.append(np.array(gram_vec).flatten())\n",
    "        text_words.append(' '.join(words))\n",
    "    text_dict = dict(zip(text_words, text_vectors))\n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b72ea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_corpus_with_ngrams(corpus_path: str,\n",
    "                                   separator: str,\n",
    "                                   word_vector_dict: dict, n : int , m : int  ) -> np.ndarray:\n",
    "    colnames=['Text Title', 'X', 'Y', 'Text'] \n",
    "    df_corpus = pd.read_csv(corpus_path, delimiter= \"$\", names =colnames, header=None)[['Text Title', 'Text']]\n",
    "    df_corpus = df_corpus.dropna(subset=['Text'])\n",
    "    corpus = df_corpus['Text'].tolist()\n",
    "    vectorized_corpus = list()\n",
    "    for text_index, text in enumerate(tqdm(corpus)):\n",
    "        vectorized_text = dict()\n",
    "        if not text:\n",
    "            continue\n",
    "        vectorized_t = convert_text_to_vector(text, word_vector_dict,n,m)\n",
    "        vectorized_text['document_index'] = text_index\n",
    "        vectorized_text['vectorized_text'] = vectorized_t\n",
    "        vectorized_corpus.append(vectorized_text)\n",
    "    return np.array(vectorized_corpus, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79d77faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How long is the n-gram\n",
    "n = 1\n",
    "#How long is a vector for every word\n",
    "m = 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c909a633",
   "metadata": {},
   "source": [
    "# Thai SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebb98281",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer():\n",
    "    def __init__(self, corp_path):\n",
    "        self.corp_path = corp_path\n",
    "    \n",
    "    def upload_corp(self):\n",
    "        with open(self.corp_path, 'r',encoding = 'utf-8') as f:\n",
    "            self.corp = list(set(f.read().split('\\n')[:-1]))\n",
    "        self.corp = [text.replace('$$$', ' ') for text in self.corp]\n",
    "    \n",
    "    \n",
    "    def log(self, part):\n",
    "        clear_output(wait=True)\n",
    "        print(f'{part} is processing')\n",
    "        \n",
    "    def make_tf_idf_matrix(self, token_pattern=None, min_df = 1, max_df=None, use_idf = True):\n",
    "        if token_pattern:\n",
    "            self.tfidf = TfidfVectorizer(analyzer='word', min_df=min_df, token_pattern=token_pattern, use_idf=use_idf)\n",
    "        else:\n",
    "            self.tfidf = TfidfVectorizer(analyzer='word', min_df=min_df, use_idf=use_idf)\n",
    "            \n",
    "        self.W = self.tfidf.fit_transform(self.corp)\n",
    "        self.words_list = self.tfidf.get_feature_names_out()\n",
    "        \n",
    "    def make_svd(self, output_folder, k=30 ):\n",
    "#         self.u, self.sigma, self.vt = svds(self.W, k)\n",
    "        \n",
    "#         self.descending_order_of_inds = np.flip(np.argsort(self.sigma))\n",
    "#         self.u = self.u[:,self.descending_order_of_inds]\n",
    "#         self.vt = self.vt[self.descending_order_of_inds,:]\n",
    "#         self.sigma = np.diag(self.sigma[self.descending_order_of_inds])\n",
    "        self.u, self.sigma, self.vT = svds(self.W, k)\n",
    "        self.descending_order_of_inds = np.argsort(-self.sigma)\n",
    "        \n",
    "        self.u = self.u[:, self.descending_order_of_inds]\n",
    "        self.sigma = np.diag(self.sigma[self.descending_order_of_inds])\n",
    "        self.vT = self.vT[self.descending_order_of_inds, :]\n",
    "\n",
    "        #Checking that sizes are ok\n",
    "        #assert self.sigma.shape == (k,)\n",
    "        #assert swlf.vt.shape == (k, self.W.shape[1])\n",
    "        #assert swlf.u.shape == (self.W.shape[0], k)\n",
    "        print (self.W.shape)\n",
    "        print (self.sigma.shape)\n",
    "        print (self.vT.shape)\n",
    "        print (self.u.shape)\n",
    "        \n",
    "        self.embedded_matrix = self.sigma@self.vT\n",
    "        #self.embedded_matrix = np.dot(np.diag(self.sigma), self.vt).T\n",
    "        self.words_embedding_dict = dict(zip(self.words_list, self.embedded_matrix.T))\n",
    "        #self.words_embedding_dict = dict(zip(self.words_list, self.embedded_matrix))\n",
    "        \n",
    "#         with open(output_folder+'/' + str(k) + '_sigma_vt.npy', 'wb') as f:\n",
    "#             #np.save(f, np.dot(np.diag(self.sigma), self.vt).T)\n",
    "#             np.save(f, self.embedded_matrix.T)\n",
    "#         with open(output_folder+'/' +  str(k) + '_sigma.npy', 'wb') as f:\n",
    "#             np.save(f, self.sigma)\n",
    "#         with open(output_folder+'/' +  str(k) + '_u.npy', 'wb') as f:\n",
    "#             np.save(f, self.u)\n",
    "#         with open(output_folder+'/' +  str(k) + '_vt.npy', 'wb') as f:\n",
    "#             np.save(f, self.vT)\n",
    "            \n",
    "        self.save_word_embedding(k)\n",
    "    \n",
    "    \n",
    "    def save_word_embedding(self, shape):\n",
    "        \n",
    "        dict_ = {}\n",
    "        for key, value in self.words_embedding_dict.items():\n",
    "            dict_[key] = value.tolist()\n",
    "        save_name = \"Thai_WordVectorDict_SVD\" + str(shape) + \".json\"\n",
    "        with open(save_name, 'w') as f:\n",
    "            json.dump(dict_, f)\n",
    "            \n",
    "    def get_emb_dict(self):\n",
    "        \n",
    "        self.log('Upload')\n",
    "        self.upload_corp()\n",
    "        self.log('TfIdf')\n",
    "        self.make_tf_idf_matrix()\n",
    "        self.log('SVD')\n",
    "        self.make_svd('./Matrixes',k=6)\n",
    "        \n",
    "        return self.words_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83483d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD is processing\n",
      "(5781, 8985590)\n",
      "(6, 6)\n",
      "(6, 8985590)\n",
      "(5781, 6)\n"
     ]
    }
   ],
   "source": [
    "vect = Vectorizer('./Thai_corpus.txt')\n",
    "emb_dict = vect.get_emb_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac91d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #How long is the n-gram\n",
    "# n = 1\n",
    "# #How long is a vector for every word\n",
    "# m = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e6baf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5878/5878 [01:31<00:00, 63.96it/s] \n"
     ]
    }
   ],
   "source": [
    "vectorized_corpus = vectorize_corpus_with_ngrams('./Thai_corpus.txt',\n",
    "                                               ' ',\n",
    "                                               emb_dict,n,int(m/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e55371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Thai_vectorized_corpus_SVD_1gram_6\", vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6b4b82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "At_VECTORIZED_CORPUS = \"./Thai_vectorized_corpus_SVD_1gram_6.npy\"\n",
    "at_corpus = np.load(At_VECTORIZED_CORPUS, allow_pickle=True)\n",
    "at_corpus = at_corpus.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e9548b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(at_corpus)/100)\n",
    "# printing n elements from list\n",
    "test = random.choices(at_corpus, k=n)\n",
    "np.save(\"Thai_vectorized_corpus_sample1%_SVD_1gram_6\", np.array(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051e80a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorized_corpus = vectorized_corpus.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d048b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertices = create_vertices(vectorized_corpus, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482dbf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63e2c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gg = GG(vertices)\n",
    "# gg.create_gabriel_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eed3f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Thai_Gabriel_Graph_SVD_1gram_100.pkl', 'wb') as outp:\n",
    "#     pickle.dump(gg, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450983b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# degrees = gg.degree_distribution()\n",
    "# dmax = max(degrees)\n",
    "\n",
    "# fig = plt.figure(\"Degree of a random graph\", figsize=(8, 8))\n",
    "# # Create a gridspec for adding subplots of different sizes\n",
    "# axgrid = fig.add_gridspec(5, 4)\n",
    "\n",
    "\n",
    "\n",
    "# ax1 = fig.add_subplot(axgrid[:, :2])\n",
    "# ax1.plot(degrees, \"b-\", marker=\"o\")\n",
    "# ax1.set_title(\"Degree Rank Plot\")\n",
    "# ax1.set_ylabel(\"Degree\")\n",
    "# ax1.set_xlabel(\"Rank\")\n",
    "\n",
    "# ax2 = fig.add_subplot(axgrid[:, 2:])\n",
    "# ax2.bar(*np.unique(degrees, return_counts=True))\n",
    "# ax2.set_title(\"Degree histogram\")\n",
    "# ax2.set_xlabel(\"Degree\")\n",
    "# ax2.set_ylabel(\"# of Nodes\")\n",
    "\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68e2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gabriel = gg.get_networkx_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ee3043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (nx.diameter(gabriel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c86a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (nx.radius(gabriel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093999da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (nx.average_shortest_path_length(gabriel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aac0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (gabriel.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09be2337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (gabriel.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404ec404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nx.is_connected(gabriel))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b447ade4",
   "metadata": {},
   "source": [
    "# Thai CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5081f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(corp_path):\n",
    "    fin = io.open(corp_path, 'r', encoding='utf-8')\n",
    "    vocab = set()\n",
    "    for line in fin:\n",
    "        for word in line.replace('$$$', ' ').split():\n",
    "            vocab.add(word)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46b93fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab('Thai_corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71a0bd69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16074599"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f603cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "dimension = 6\n",
    "model = Word2Vec(vector_size=dimension, min_count=1)\n",
    "model.build_vocab(vocab)\n",
    "model.save(\"word2vec_Thai.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "318ee09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with io.open('Thai_corpus.txt', 'r', encoding='utf-8') as file:\n",
    "    eof = False\n",
    "    while not eof:\n",
    "        limit = 1000\n",
    "        documents = []\n",
    "        for line in file:\n",
    "            documents.append(line.split())\n",
    "            limit -= 1\n",
    "            if limit == 0:\n",
    "                break\n",
    "        else:\n",
    "            eof = True\n",
    "        model = Word2Vec.load(\"word2vec_Thai.model\")\n",
    "        model.build_vocab(documents, update=True)\n",
    "        model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "        model.save(\"word2vec_Thai.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fbba2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dict = {key : model.wv[key] for key in model.wv.key_to_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3366c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(emb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a662718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #How long is the n-gram\n",
    "# n = 1\n",
    "# #How long is a vector for every word\n",
    "# m = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfd9a22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5878/5878 [04:13<00:00, 23.16it/s]\n"
     ]
    }
   ],
   "source": [
    "vectorized_corpus = vectorize_corpus_with_ngrams('./Thai_corpus.txt',\n",
    "                                               ' ',\n",
    "                                               emb_dict,n,int(m/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "068d3e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Thai_vectorized_corpus_CBOW_1gram_6\", vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75465634",
   "metadata": {},
   "outputs": [],
   "source": [
    "At_VECTORIZED_CORPUS = \"./Thai_vectorized_corpus_CBOW_1gram_6.npy\"\n",
    "at_corpus = np.load(At_VECTORIZED_CORPUS, allow_pickle=True)\n",
    "at_corpus = at_corpus.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a19b2f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(at_corpus)/100)\n",
    "# printing n elements from list\n",
    "test = random.choices(at_corpus, k=n)\n",
    "np.save(\"Thai_vectorized_corpus_sample1%_CBOW_1gram_6\", np.array(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d429b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorized_corpus = vectorized_corpus.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c2769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertices = create_vertices(vectorized_corpus, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0afe341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16d4169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gg = GG(vertices)\n",
    "# gg.create_gabriel_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da882252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Thai_Gabriel_Graph_CBOW_1gram_100.pkl', 'wb') as outp:\n",
    "#     pickle.dump(gg, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08328d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# degrees = gg.degree_distribution()\n",
    "# dmax = max(degrees)\n",
    "\n",
    "# fig = plt.figure(\"Degree of a random graph\", figsize=(8, 8))\n",
    "# # Create a gridspec for adding subplots of different sizes\n",
    "# axgrid = fig.add_gridspec(5, 4)\n",
    "\n",
    "\n",
    "\n",
    "# ax1 = fig.add_subplot(axgrid[:, :2])\n",
    "# ax1.plot(degrees, \"b-\", marker=\"o\")\n",
    "# ax1.set_title(\"Degree Rank Plot\")\n",
    "# ax1.set_ylabel(\"Degree\")\n",
    "# ax1.set_xlabel(\"Rank\")\n",
    "\n",
    "# ax2 = fig.add_subplot(axgrid[:, 2:])\n",
    "# ax2.bar(*np.unique(degrees, return_counts=True))\n",
    "# ax2.set_title(\"Degree histogram\")\n",
    "# ax2.set_xlabel(\"Degree\")\n",
    "# ax2.set_ylabel(\"# of Nodes\")\n",
    "\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5eb4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gabriel = gg.get_networkx_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079450ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (nx.diameter(gabriel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad27f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (nx.radius(gabriel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c568fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (nx.average_shortest_path_length(gabriel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868d7913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (gabriel.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391795a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (gabriel.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fe245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nx.is_connected(gabriel))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
