{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be0e5ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f2144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_corpus(input_path, output_file_path):\n",
    "    file_list = sorted(glob.glob(input_path + '/*'))\n",
    "    with open(output_file_path, 'a', encoding = 'utf-8') as output_file:\n",
    "        for file in tqdm.tqdm(file_list):\n",
    "            with open(file, 'r', encoding = 'utf-8') as input_file:\n",
    "                #file_name_full  = file.split('\\\\')[-1]\n",
    "                #file_name_without_proces = file_name_full.split('_')[1]\n",
    "                #file_name_without_txt = file_name_without_proces.split('.')[0]\n",
    "                file_content = input_file.read()\n",
    "                file_content = file_content.replace('\\n', ' ')\n",
    "                #output_file.write(file_name_without_txt)\n",
    "                #output_file.write(\"$$$\")\n",
    "                output_file.write(file_content)\n",
    "                output_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257154c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_corpus (\"prepared_dataset\" , \"Arabic_corpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45823297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse.linalg import svds\n",
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba382346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f73b69c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569403e4",
   "metadata": {},
   "source": [
    "# make SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372ed460",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer():\n",
    "    def __init__(self, corp_path):\n",
    "        self.corp_path = corp_path\n",
    "    \n",
    "    def upload_corp(self):\n",
    "        with open(self.corp_path, 'r',encoding = 'utf-8') as f:\n",
    "            self.corp = list(set(f.read().split('\\n')[:-1]))\n",
    "        #self.corp = [text.replace('$$$', ' ') for text in self.corp]\n",
    "    \n",
    "    \n",
    "    def log(self, part):\n",
    "        clear_output(wait=True)\n",
    "        print(f'{part} is processing')\n",
    "        \n",
    "    def make_tf_idf_matrix(self, token_pattern=None, min_df = 1, max_df=None, use_idf = True):\n",
    "        if token_pattern:\n",
    "            self.tfidf = TfidfVectorizer(analyzer='word', min_df=min_df, token_pattern=token_pattern, use_idf=use_idf)\n",
    "        else:\n",
    "            self.tfidf = TfidfVectorizer(analyzer='word', min_df=min_df, use_idf=use_idf)\n",
    "            \n",
    "        self.W = self.tfidf.fit_transform(self.corp)\n",
    "        self.words_list = self.tfidf.get_feature_names_out()\n",
    "        \n",
    "    def make_svd(self, output_folder, k=6 ):\n",
    "#         self.u, self.sigma, self.vt = svds(self.W, k)\n",
    "        \n",
    "#         self.descending_order_of_inds = np.flip(np.argsort(self.sigma))\n",
    "#         self.u = self.u[:,self.descending_order_of_inds]\n",
    "#         self.vt = self.vt[self.descending_order_of_inds,:]\n",
    "#         self.sigma = np.diag(self.sigma[self.descending_order_of_inds])\n",
    "        self.u, self.sigma, self.vT = svds(self.W, k)\n",
    "        self.descending_order_of_inds = np.argsort(-self.sigma)\n",
    "        \n",
    "        self.u = self.u[:, self.descending_order_of_inds]\n",
    "        self.sigma = np.diag(self.sigma[self.descending_order_of_inds])\n",
    "        self.vT = self.vT[self.descending_order_of_inds, :]\n",
    "\n",
    "        #Checking that sizes are ok\n",
    "        #assert self.sigma.shape == (k,)\n",
    "        #assert swlf.vt.shape == (k, self.W.shape[1])\n",
    "        #assert swlf.u.shape == (self.W.shape[0], k)\n",
    "        print (self.W.shape)\n",
    "        print (self.sigma.shape)\n",
    "        print (self.vT.shape)\n",
    "        print (self.u.shape)\n",
    "        \n",
    "        self.embedded_matrix = self.sigma@self.vT\n",
    "        #self.embedded_matrix = np.dot(np.diag(self.sigma), self.vt).T\n",
    "        self.words_embedding_dict = dict(zip(self.words_list, self.embedded_matrix.T))\n",
    "        #self.words_embedding_dict = dict(zip(self.words_list, self.embedded_matrix))\n",
    "        \n",
    "#         with open(output_folder+'/' + str(k) + '_sigma_vt.npy', 'wb') as f:\n",
    "#             #np.save(f, np.dot(np.diag(self.sigma), self.vt).T)\n",
    "#             np.save(f, self.embedded_matrix.T)\n",
    "#         with open(output_folder+'/' +  str(k) + '_sigma.npy', 'wb') as f:\n",
    "#             np.save(f, self.sigma)\n",
    "#         with open(output_folder+'/' +  str(k) + '_u.npy', 'wb') as f:\n",
    "#             np.save(f, self.u)\n",
    "#         with open(output_folder+'/' +  str(k) + '_vt.npy', 'wb') as f:\n",
    "#             np.save(f, self.vT)\n",
    "            \n",
    "        self.save_word_embedding(k)\n",
    "    \n",
    "    \n",
    "    def get_emb_dict(self):\n",
    "        \n",
    "        self.log('Upload')\n",
    "        self.upload_corp()\n",
    "        self.log('TfIdf')\n",
    "        self.make_tf_idf_matrix()\n",
    "        self.log('SVD')\n",
    "        self.make_svd('./Matrixes')\n",
    "        \n",
    "        return self.words_embedding_dict\n",
    "    \n",
    "    def save_word_embedding(self, shape):\n",
    "        \n",
    "        dict_ = {}\n",
    "        for key, value in self.words_embedding_dict.items():\n",
    "            dict_[key] = value.tolist()\n",
    "        save_name = \"Arabic_WordVectorDict_SVD\" + str(shape) + \".json\"\n",
    "        with open(save_name, 'w') as f:\n",
    "            json.dump(dict_, f)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57672dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = Vectorizer('./Arabic_corpus.txt')\n",
    "emb_dict = vect.get_emb_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3335b7e",
   "metadata": {},
   "source": [
    "# make CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c27a35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(corp_path):\n",
    "    with open(corp_path, 'r',encoding = 'utf-8') as f:\n",
    "        corp = list(set(f.read().split('\\n')[:-1]))\n",
    "    #corp = [text.replace('$$$', ' ') for text in corp]\n",
    "    corp = [text.split() for text in corp]\n",
    "    return corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a86d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_corpus('Arabic_corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd22856",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e08ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 6\n",
    "model = Word2Vec(sentences=documents, vector_size=dimension, min_count=1)\n",
    "model.save(\"word2vec_Arabic.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da514a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {key : model.wv[key] for key in model.wv.key_to_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d851736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dictionary(dictionary,shape):\n",
    "    for key, value in dictionary.items():\n",
    "        dictionary[key] = value.tolist()\n",
    "    save_name = \"Arabic_WordVectorDict_CBOW\" + str(shape) + \".json\"\n",
    "    with open(save_name, 'w') as f:\n",
    "        json.dump(dictionary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1c6c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dictionary( dictionary, dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf2d7f0",
   "metadata": {},
   "source": [
    "# vectorized corpuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2acf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = './Arabic_corpus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2246cd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_vector(text, dictionary, n, m):\n",
    "    text = text.strip().split()\n",
    "    text_vectors = []\n",
    "    text_words = []\n",
    "\n",
    "    for i in range(len(text) - n + 1):\n",
    "        gram_vec = []\n",
    "        words = []\n",
    "        for word in text[i:i+n]:\n",
    "            if word not in dictionary:\n",
    "                gram_vec = []\n",
    "                break\n",
    "            vec_ = dictionary[word][:m]\n",
    "            gram_vec.append(vec_)\n",
    "            words.append(word)\n",
    "        if len(gram_vec) != n or len(gram_vec[-1]) != m:\n",
    "            continue\n",
    "        text_vectors.append(np.array(gram_vec).flatten())\n",
    "        text_words.append(' '.join(words))\n",
    "    text_dict = dict(zip(text_words, text_vectors))\n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eafa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_corpus_with_ngrams(corpus_path: str,\n",
    "                                   separator: str,\n",
    "                                   word_vector_dict: dict, n : int , m : int  ) -> np.ndarray:\n",
    "    colnames=[ 'Text'] \n",
    "    df_corpus = pd.read_csv(corpus_path, names =colnames, header=None)\n",
    "    corpus = df_corpus['Text'].tolist()\n",
    "    vectorized_corpus = list()\n",
    "    for text_index, text in enumerate(tqdm(corpus)):\n",
    "        vectorized_text = dict()\n",
    "        if not text:\n",
    "            continue\n",
    "        vectorized_t = convert_text_to_vector(text, word_vector_dict,n,m)\n",
    "        vectorized_text['document_index'] = text_index\n",
    "        vectorized_text['vectorized_text'] = vectorized_t\n",
    "        vectorized_corpus.append(vectorized_text)\n",
    "    return np.array(vectorized_corpus, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a17c421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How long is the n-gram\n",
    "n = 1\n",
    "#How long is a vector for every word\n",
    "m = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dfcb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Arabic_WordVectorDict_CBOW6.json', encoding='utf-8') as f:\n",
    "    emb_dic = json.load(f)\n",
    "#print(CBOW_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c82c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emb_dic['التي'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be23fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_text_to_vector('والالقاب التي حققها طيله مسيرته', emb_dic, n, int(m/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fb5ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = vectorize_corpus_with_ngrams(corpus_path,\n",
    "                                               ' ',\n",
    "                                               emb_dic,n,int(m/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf81590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Arabic_vectorized_corpus_CBOW_1gram_6\", vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33b071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Arabic_WordVectorDict_SVD6.json\", encoding='utf-8') as f:\n",
    "    emb_dic = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1525620",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = vectorize_corpus_with_ngrams(corpus_path,\n",
    "                                               ' ',\n",
    "                                               emb_dic,n,int(m/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d791a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Arabic_vectorized_corpus_SVD_1gram_6\", vectorized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcae5d6",
   "metadata": {},
   "source": [
    " # Arabic Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e969060",
   "metadata": {},
   "outputs": [],
   "source": [
    "At_VECTORIZED_CORPUS = \"./Arabic_vectorized_corpus_SVD_1gram_6.npy\"\n",
    "at_corpus = np.load(At_VECTORIZED_CORPUS, allow_pickle=True)\n",
    "at_corpus = at_corpus.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37def0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2912"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(at_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5be5ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "n = int(len(at_corpus)/100)\n",
    "# printing n elements from list\n",
    "test = random.choices(at_corpus, k=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0774139d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "print (len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d89fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the sample \n",
    "#np.save(\"Arabic_vectorized_corpus_sample10%_SVD_1gram_6\", np.array(test))\n",
    "np.save(\"Arabic_vectorized_corpus_sample1%_SVD_1gram_6\", np.array(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e95c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "At_VECTORIZED_CORPUS = \"./Arabic_vectorized_corpus_sample10%_SVD_1gram_6.npy\"\n",
    "at_corpus = np.load(At_VECTORIZED_CORPUS, allow_pickle=True)\n",
    "at_corpus = at_corpus.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb26e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = create_vertices(at_corpus, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8a1ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef4ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = GG(vertices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df6631",
   "metadata": {},
   "outputs": [],
   "source": [
    "gg.reset_graph_neighbors()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39df961",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = gg.get_vectors()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885fd431",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = gg.get_words()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cf5d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_num_dict = {word: num for word, num in enumerate(words)}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c9449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ = open('eng_delaunay_progress.txt', 'w')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b631bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_.write(\"Delaunay start... \\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gg.delaunay = Delaunay(np.array(vectors), qhull_options=\"Qbb Qc Qz Qx Q12\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f241450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "delaunay_graph = gg.delaunay.simplices.tolist()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc19b48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_.write(\"Delaunay done!\")\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2746c3",
   "metadata": {},
   "outputs": [],
   "source": [
    " for triangle in tqdm(delaunay_graph, file=file_):\n",
    "            triangle_words = set(map(word_num_dict.get, triangle))\n",
    "            self.triangles.append(triangle_words)\n",
    "            for word in triangle_words:\n",
    "                new_neighbors = triangle_words.difference(set([word]))\n",
    "                self.vertices[word].neighbors.update(new_neighbors)\n",
    "    file_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6b2d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gg.create_gabriel_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966098b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Arabic_Gabriel_Graph_SVD_1gram_6.pkl', 'wb') as outp:\n",
    "    pickle.dump(gg, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c839be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Arabic_Gabriel_Graph_SVD_1gram_6.pkl', 'rb') as inp:\n",
    "#     At_graph_svd_1g6 = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acc577e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "At_VECTORIZED_CORPUS = \"./Arabic_vectorized_corpus_CBOW_1gram_6.npy\"\n",
    "at_corpus = np.load(At_VECTORIZED_CORPUS, allow_pickle=True)\n",
    "at_corpus = at_corpus.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ec7a84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "n = int(len(at_corpus)/100)\n",
    "# printing n elements from list\n",
    "test = random.choices(at_corpus, k=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "448a5c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the sample \n",
    "#np.save(\"Arabic_vectorized_corpus_sample10%_CBOW_1gram_6\", np.array(test))\n",
    "np.save(\"Arabic_vectorized_corpus_sample1%_CBOW_1gram_6\", np.array(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16ee6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = create_vertices(at_corpus, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4599cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = GG(vertices)\n",
    "gg.create_gabriel_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8dd673",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Arabic_Gabriel_Graph_CBOW_1gram_6.pkl', 'wb') as outp:\n",
    "    pickle.dump(gg, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca79ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Arabic_Gabriel_Graph_CBOW_1gram_6.pkl', 'rb') as inp:\n",
    "#     At_graph_svd_1g6 = pickle.load(inp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
