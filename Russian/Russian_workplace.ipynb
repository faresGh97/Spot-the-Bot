{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9cb5dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tqdm\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6424981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(input_path,output_path):\n",
    "    fin = open(input_path, \"rt\",encoding = 'utf-8')\n",
    "    fout = open(output_path, \"wt\",encoding = 'utf-8')\n",
    "    for line in fin:\n",
    "        \n",
    "        \n",
    "        line = line.replace(\"of \", \"\")\n",
    "        line = line.replace(\"body \", \"\")\n",
    "        line = line.replace(\"img \", \"\")\n",
    "        line = line.replace(\"% max-width \", \"\")\n",
    "        line = line.replace(\"% div center page-break-after always\", \"\")\n",
    "        line = line.replace(\" } \", \"\")\n",
    "        fout.write(line)\n",
    "        #break\n",
    "        #fout.write(re.sub('\\s+',' ',line))\n",
    "    fin.close()\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e738320",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus(\"russian_nofraglit_corpus.txt\",\"Russian_corpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "064484bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse.linalg import svds\n",
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd \n",
    "from gensim.models import Word2Vec\n",
    "from graph_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ad0d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer():\n",
    "    def __init__(self, corp_path):\n",
    "        self.corp_path = corp_path\n",
    "    \n",
    "    def upload_corp(self):\n",
    "        with open(self.corp_path, 'r',encoding = 'utf-8') as f:\n",
    "            self.corp = list(set(f.read().split('\\n')[:-1]))\n",
    "        #self.corp = [text.replace('$$$', ' ') for text in self.corp]\n",
    "    \n",
    "    \n",
    "    def log(self, part):\n",
    "        clear_output(wait=True)\n",
    "        print(f'{part} is processing')\n",
    "        \n",
    "    def make_tf_idf_matrix(self, token_pattern=None, min_df = 1, max_df=None, use_idf = True):\n",
    "        if token_pattern:\n",
    "            self.tfidf = TfidfVectorizer(analyzer='word', min_df=min_df, token_pattern=token_pattern, use_idf=use_idf)\n",
    "        else:\n",
    "            self.tfidf = TfidfVectorizer(analyzer='word', min_df=min_df, use_idf=use_idf)\n",
    "            \n",
    "        self.W = self.tfidf.fit_transform(self.corp)\n",
    "        self.words_list = self.tfidf.get_feature_names_out()\n",
    "        \n",
    "    def make_svd(self, output_folder, k=6 ):\n",
    "#         self.u, self.sigma, self.vt = svds(self.W, k)\n",
    "        \n",
    "#         self.descending_order_of_inds = np.flip(np.argsort(self.sigma))\n",
    "#         self.u = self.u[:,self.descending_order_of_inds]\n",
    "#         self.vt = self.vt[self.descending_order_of_inds,:]\n",
    "#         self.sigma = np.diag(self.sigma[self.descending_order_of_inds])\n",
    "        self.u, self.sigma, self.vT = svds(self.W, k)\n",
    "        self.descending_order_of_inds = np.argsort(-self.sigma)\n",
    "        \n",
    "        self.u = self.u[:, self.descending_order_of_inds]\n",
    "        self.sigma = np.diag(self.sigma[self.descending_order_of_inds])\n",
    "        self.vT = self.vT[self.descending_order_of_inds, :]\n",
    "\n",
    "        #Checking that sizes are ok\n",
    "        #assert self.sigma.shape == (k,)\n",
    "        #assert swlf.vt.shape == (k, self.W.shape[1])\n",
    "        #assert swlf.u.shape == (self.W.shape[0], k)\n",
    "        print (self.W.shape)\n",
    "        print (self.sigma.shape)\n",
    "        print (self.vT.shape)\n",
    "        print (self.u.shape)\n",
    "        \n",
    "        self.embedded_matrix = self.sigma@self.vT\n",
    "        #self.embedded_matrix = np.dot(np.diag(self.sigma), self.vt).T\n",
    "        self.words_embedding_dict = dict(zip(self.words_list, self.embedded_matrix.T))\n",
    "        #self.words_embedding_dict = dict(zip(self.words_list, self.embedded_matrix))\n",
    "        \n",
    "#         with open(output_folder+'/' + str(k) + '_sigma_vt.npy', 'wb') as f:\n",
    "#             #np.save(f, np.dot(np.diag(self.sigma), self.vt).T)\n",
    "#             np.save(f, self.embedded_matrix.T)\n",
    "#         with open(output_folder+'/' +  str(k) + '_sigma.npy', 'wb') as f:\n",
    "#             np.save(f, self.sigma)\n",
    "#         with open(output_folder+'/' +  str(k) + '_u.npy', 'wb') as f:\n",
    "#             np.save(f, self.u)\n",
    "#         with open(output_folder+'/' +  str(k) + '_vt.npy', 'wb') as f:\n",
    "#             np.save(f, self.vT)\n",
    "            \n",
    "        self.save_word_embedding(k)\n",
    "    \n",
    "    \n",
    "    def get_emb_dict(self):\n",
    "        \n",
    "        self.log('Upload')\n",
    "        self.upload_corp()\n",
    "        self.log('TfIdf')\n",
    "        self.make_tf_idf_matrix()\n",
    "        self.log('SVD')\n",
    "        self.make_svd('./Matrixes')\n",
    "        \n",
    "        return self.words_embedding_dict\n",
    "    \n",
    "    def save_word_embedding(self, shape):\n",
    "        \n",
    "        dict_ = {}\n",
    "        for key, value in self.words_embedding_dict.items():\n",
    "            dict_[key] = value.tolist()\n",
    "        save_name = \"Russian_WordVectorDict_SVD\" + str(shape) + \".json\"\n",
    "        with open(save_name, 'w') as f:\n",
    "            json.dump(dict_, f)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f900b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD is processing\n",
      "(6423, 589723)\n",
      "(6, 6)\n",
      "(6, 589723)\n",
      "(6423, 6)\n"
     ]
    }
   ],
   "source": [
    "vect = Vectorizer('./Russian_corpus.txt')\n",
    "emb_dict = vect.get_emb_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1ece7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(corp_path):\n",
    "    with open(corp_path, 'r',encoding = 'utf-8') as f:\n",
    "        corp = list(set(f.read().split('\\n')[:-1]))\n",
    "    #corp = [text.replace('$$$', ' ') for text in corp]\n",
    "    corp = [text.split() for text in corp]\n",
    "    return corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b115841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_corpus('Russian_corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a780e31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6423"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40ae57b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 6\n",
    "model = Word2Vec(sentences=documents, vector_size=dimension, min_count=1)\n",
    "model.save(\"word2vec_Russian.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e47bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {key : model.wv[key] for key in model.wv.key_to_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54e014d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dictionary(dictionary,shape):\n",
    "    for key, value in dictionary.items():\n",
    "        dictionary[key] = value.tolist()\n",
    "    save_name = \"Russian_WordVectorDict_CBOW\" + str(shape) + \".json\"\n",
    "    with open(save_name, 'w') as f:\n",
    "        json.dump(dictionary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62031d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dictionary( dictionary, dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f3e8730",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = './Russian_corpus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "682bf310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_vector(text, dictionary, n, m):\n",
    "    text = text.strip().split()\n",
    "    text_vectors = []\n",
    "    text_words = []\n",
    "\n",
    "    for i in range(len(text) - n + 1):\n",
    "        gram_vec = []\n",
    "        words = []\n",
    "        for word in text[i:i+n]:\n",
    "            if word not in dictionary:\n",
    "                gram_vec = []\n",
    "                break\n",
    "            vec_ = dictionary[word][:m]\n",
    "            gram_vec.append(vec_)\n",
    "            words.append(word)\n",
    "        if len(gram_vec) != n or len(gram_vec[-1]) != m:\n",
    "            continue\n",
    "        text_vectors.append(np.array(gram_vec).flatten())\n",
    "        text_words.append(' '.join(words))\n",
    "    text_dict = dict(zip(text_words, text_vectors))\n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6401fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_corpus_with_ngrams(corpus_path: str,\n",
    "                                   separator: str,\n",
    "                                   word_vector_dict: dict, n : int , m : int  ) -> np.ndarray:\n",
    "    colnames=[ 'Text'] \n",
    "    df_corpus = pd.read_csv(corpus_path,delimiter= \"(\", names =colnames, header=None)\n",
    "    corpus = df_corpus['Text'].tolist()\n",
    "    vectorized_corpus = list()\n",
    "    for text_index, text in enumerate(tqdm(corpus)):\n",
    "        vectorized_text = dict()\n",
    "        if not text:\n",
    "            continue\n",
    "        vectorized_t = convert_text_to_vector(text, word_vector_dict,n,m)\n",
    "        vectorized_text['document_index'] = text_index\n",
    "        vectorized_text['vectorized_text'] = vectorized_t\n",
    "        vectorized_corpus.append(vectorized_text)\n",
    "    return np.array(vectorized_corpus, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c447ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How long is the n-gram\n",
    "n = 1\n",
    "#How long is a vector for every word\n",
    "m = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05c46578",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Russian_WordVectorDict_CBOW6.json', encoding='utf-8') as f:\n",
    "    emb_dic = json.load(f)\n",
    "#print(CBOW_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5d40658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6429/6429 [02:47<00:00, 38.40it/s]\n"
     ]
    }
   ],
   "source": [
    "vectorized_corpus = vectorize_corpus_with_ngrams(corpus_path,\n",
    "                                               ' ',\n",
    "                                               emb_dic,n,int(m/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0aa41132",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Russian_vectorized_corpus_CBOW_1gram_6\", vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e336a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Russian_WordVectorDict_SVD6.json\", encoding='utf-8') as f:\n",
    "    emb_dic = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "963bbded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6429/6429 [02:25<00:00, 44.13it/s] \n"
     ]
    }
   ],
   "source": [
    "vectorized_corpus = vectorize_corpus_with_ngrams(corpus_path,\n",
    "                                               ' ',\n",
    "                                               emb_dic,n,int(m/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce315c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Russian_vectorized_corpus_SVD_1gram_6\", vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeb621dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "At_VECTORIZED_CORPUS = \"./Russian_vectorized_corpus_SVD_1gram_6.npy\"\n",
    "at_corpus = np.load(At_VECTORIZED_CORPUS, allow_pickle=True)\n",
    "at_corpus = at_corpus.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3baba92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6429"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(at_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31d42c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(at_corpus)/100)\n",
    "# printing n elements from list\n",
    "test = random.choices(at_corpus, k=n)\n",
    "np.save(\"Russian_vectorized_corpus_sample1%_SVD_1gram_6\", np.array(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "898fa401",
   "metadata": {},
   "outputs": [],
   "source": [
    "At_VECTORIZED_CORPUS = \"./Russian_vectorized_corpus_CBOW_1gram_6.npy\"\n",
    "at_corpus = np.load(At_VECTORIZED_CORPUS, allow_pickle=True)\n",
    "at_corpus = at_corpus.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a7fa68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6429"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(at_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "237cf16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(at_corpus)/100)\n",
    "# printing n elements from list\n",
    "test = random.choices(at_corpus, k=n)\n",
    "np.save(\"Russian_vectorized_corpus_sample1%_CBOW_1gram_6\", np.array(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0335d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
